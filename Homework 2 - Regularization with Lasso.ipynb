{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "In this homework, you'll be required to load in a dataset which has about 500 features. By using\n",
    "Lasso ($L^1$) regression, we'll find the optimal constraint on the $L^1$ norm which gives us the best\n",
    "$R^2$. Then we'll plot the results.\n",
    "\n",
    "Recall we minimize the following on ** training data: $(x_i,y_i)$**\n",
    "\n",
    "$$\\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta \\cdot x_i)^2 + \\lambda \\|\\beta \\|_{L^1}.$$\n",
    "\n",
    "\n",
    "Denoting $\\beta_{\\lambda}$ as the minimum of the above, we then choose $\\lambda$ to minimize $R^2$ on **testing data: $(x_j,y_j)$**\n",
    "\n",
    "$$ \\min_{\\lambda} 1 - \\frac{\\sum_{j} (y_j - \\beta_{\\lambda} \\cdot x_j)^2}{\\sum_j (y_j - \\bar y)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load in hw2data.csv from ../data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.382732</td>\n",
       "      <td>-0.034242</td>\n",
       "      <td>1.096347</td>\n",
       "      <td>-0.234216</td>\n",
       "      <td>-0.347451</td>\n",
       "      <td>-0.581268</td>\n",
       "      <td>-1.632635</td>\n",
       "      <td>-1.567768</td>\n",
       "      <td>-1.179158</td>\n",
       "      <td>1.301428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178793</td>\n",
       "      <td>-0.799422</td>\n",
       "      <td>0.240788</td>\n",
       "      <td>0.289121</td>\n",
       "      <td>0.412871</td>\n",
       "      <td>-0.198399</td>\n",
       "      <td>0.094192</td>\n",
       "      <td>-1.147611</td>\n",
       "      <td>-0.358114</td>\n",
       "      <td>-2.663126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555963</td>\n",
       "      <td>0.892474</td>\n",
       "      <td>-0.422315</td>\n",
       "      <td>0.104714</td>\n",
       "      <td>0.228053</td>\n",
       "      <td>0.201480</td>\n",
       "      <td>0.540774</td>\n",
       "      <td>-1.818078</td>\n",
       "      <td>-0.049324</td>\n",
       "      <td>0.239034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.740137</td>\n",
       "      <td>-0.565498</td>\n",
       "      <td>0.476031</td>\n",
       "      <td>-2.158069</td>\n",
       "      <td>1.318551</td>\n",
       "      <td>-0.239297</td>\n",
       "      <td>-0.246794</td>\n",
       "      <td>-1.079343</td>\n",
       "      <td>-0.114226</td>\n",
       "      <td>10.399650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013240</td>\n",
       "      <td>-0.121945</td>\n",
       "      <td>0.339059</td>\n",
       "      <td>-0.589632</td>\n",
       "      <td>-0.895816</td>\n",
       "      <td>0.548328</td>\n",
       "      <td>0.098667</td>\n",
       "      <td>0.197181</td>\n",
       "      <td>1.059027</td>\n",
       "      <td>-1.022564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.739936</td>\n",
       "      <td>1.315138</td>\n",
       "      <td>-0.323457</td>\n",
       "      <td>0.197828</td>\n",
       "      <td>0.097751</td>\n",
       "      <td>1.401523</td>\n",
       "      <td>0.158434</td>\n",
       "      <td>-1.141901</td>\n",
       "      <td>-1.310970</td>\n",
       "      <td>-21.762801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.532921</td>\n",
       "      <td>-1.711970</td>\n",
       "      <td>0.046135</td>\n",
       "      <td>-0.958374</td>\n",
       "      <td>-0.080812</td>\n",
       "      <td>-0.703859</td>\n",
       "      <td>-0.770784</td>\n",
       "      <td>-0.480845</td>\n",
       "      <td>0.703586</td>\n",
       "      <td>0.929145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473488</td>\n",
       "      <td>1.855246</td>\n",
       "      <td>1.415656</td>\n",
       "      <td>-0.302746</td>\n",
       "      <td>0.989679</td>\n",
       "      <td>0.585851</td>\n",
       "      <td>1.136388</td>\n",
       "      <td>0.671617</td>\n",
       "      <td>-0.974167</td>\n",
       "      <td>2.139453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.619685</td>\n",
       "      <td>0.572627</td>\n",
       "      <td>1.902618</td>\n",
       "      <td>-0.775664</td>\n",
       "      <td>-0.188090</td>\n",
       "      <td>-1.035748</td>\n",
       "      <td>1.177830</td>\n",
       "      <td>-2.305167</td>\n",
       "      <td>-2.263660</td>\n",
       "      <td>0.375020</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.303220</td>\n",
       "      <td>0.466751</td>\n",
       "      <td>0.161106</td>\n",
       "      <td>0.320032</td>\n",
       "      <td>2.079177</td>\n",
       "      <td>-0.907466</td>\n",
       "      <td>-0.192404</td>\n",
       "      <td>-1.212516</td>\n",
       "      <td>-0.080599</td>\n",
       "      <td>0.194017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.382732 -0.034242  1.096347 -0.234216 -0.347451 -0.581268 -1.632635   \n",
       "1  0.555963  0.892474 -0.422315  0.104714  0.228053  0.201480  0.540774   \n",
       "2  0.013240 -0.121945  0.339059 -0.589632 -0.895816  0.548328  0.098667   \n",
       "3 -1.532921 -1.711970  0.046135 -0.958374 -0.080812 -0.703859 -0.770784   \n",
       "4 -1.619685  0.572627  1.902618 -0.775664 -0.188090 -1.035748  1.177830   \n",
       "\n",
       "          7         8         9    ...           491       492       493  \\\n",
       "0 -1.567768 -1.179158  1.301428    ...      0.178793 -0.799422  0.240788   \n",
       "1 -1.818078 -0.049324  0.239034    ...     -0.740137 -0.565498  0.476031   \n",
       "2  0.197181  1.059027 -1.022564    ...     -0.739936  1.315138 -0.323457   \n",
       "3 -0.480845  0.703586  0.929145    ...      0.473488  1.855246  1.415656   \n",
       "4 -2.305167 -2.263660  0.375020    ...     -1.303220  0.466751  0.161106   \n",
       "\n",
       "        494       495       496       497       498       499          y  \n",
       "0  0.289121  0.412871 -0.198399  0.094192 -1.147611 -0.358114  -2.663126  \n",
       "1 -2.158069  1.318551 -0.239297 -0.246794 -1.079343 -0.114226  10.399650  \n",
       "2  0.197828  0.097751  1.401523  0.158434 -1.141901 -1.310970 -21.762801  \n",
       "3 -0.302746  0.989679  0.585851  1.136388  0.671617 -0.974167   2.139453  \n",
       "4  0.320032  2.079177 -0.907466 -0.192404 -1.212516 -0.080599   0.194017  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/hw2data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Set to be the y variable in the dataframe from a and X to be the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 500)\n",
      "(225,)\n"
     ]
    }
   ],
   "source": [
    "y = df['y']\n",
    "X = df.drop(['y'],1)\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As shown in the Booking.com example, using Lasso regression, find the regularization strength\n",
    "which optimizes the $R^2$. \n",
    "\n",
    "**Hint:** Take a range of alpha from `np.logspace(-8,-3,1000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mars/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regularization parameter : 0.000691575882874\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOryv8HlFnGPGKO0dkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2ebc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#standardize data\n",
    "scaler = StandardScaler()\n",
    "XX = X.as_matrix().astype(np.float)\n",
    "XX = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "# Create linear regression object\n",
    "alphas = np.logspace(-8,-3,1000)\n",
    "from sklearn.linear_model import Lasso\n",
    "scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    regr = Lasso(alpha=alpha, max_iter = 10000)\n",
    "    regr.fit(X_train, y_train)\n",
    "    scores.append(regr.score(X_test,y_test))\n",
    "    \n",
    "plt.plot(alphas,scores)\n",
    "plt.xlabel('alphas')\n",
    "plt.ylabel('scores')\n",
    "i_alpha_optim = np.argmax(scores)\n",
    "alpha_optim = alphas[i_alpha_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % alpha_optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Plot the training perforamnce versus the testing performance, and observe whree the test performance is\n",
    "maximized. I've written an outline of the code you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "for alpha in alphas:\n",
    "    regr = Lasso(alpha=alpha, max_iter = 10000\n",
    "#                 , fit_intercept = True, normalize = True\n",
    "                )\n",
    "    regr.fit(X_train, y_train)\n",
    "    train_errors.append(regr.score(X_train, y_train))\n",
    "    test_errors.append(regr.score(X_test, y_test))\n",
    "alpha_optim=0\n",
    "i_alpha_optim = np.argmax(test_errors)\n",
    "alpha_optim = alphas[i_alpha_optim]\n",
    "print (\"Optimal regularization parameter : %s\" % alpha_optim)\n",
    "\n",
    "plt.semilogx(alphas, train_errors, label='Train')\n",
    "plt.semilogx(alphas, test_errors, label='Test')\n",
    "plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = Lasso(alpha=alpha_optim)\n",
    "regr.fit(X_train,y_train)\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')\n",
    "plt.bar(range(500),np.sort(regr.coef_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Because Lasso regression, $\\left| \\beta_{1} \\right| + \\left| \\beta_{2} \\right| = C$, the level set of is most likely to be tangent to the level sets at a corner. Since there are only 4 possible directions where both of the coefficients are non-zero, making it highly unlikely that the level sets of L has a tangent parallel to any one of these directions. Therefore, it result in so many zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Compute the $R^2$ with the optimal coefficient found above on 5 folds using cross_val_score and plot the\n",
    "results. Does the model work well on all random subsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha_best = alpha_optim\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regr = Lasso(alpha = alpha_optim, \n",
    "#              fit_intercept = True, normalize = True\n",
    "            )\n",
    "scores = cross_val_score(regr, X, y, cv=5)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('Performance on 5 folds with lambda=' + str(alpha_optim))\n",
    "plt.bar(range(1,6),scores)\n",
    "plt.show()\n",
    "print (\"The scores for optimal alpha is: %s\" %scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth()*4)\n",
    "\n",
    "# Construct training and testing set.\n",
    "for m in xrange(4):\n",
    "    X, y = shuffle(X, y)\n",
    "    scores = cross_val_score(regr, X, y, cv=5)\n",
    "    axes = fig.add_subplot(1, 4, m+1)\n",
    "    axes.set_xlabel('lambda')\n",
    "    axes.set_ylabel('R^2')\n",
    "    axes.set_title('Shuffle random subsets: ' + str(m+1))\n",
    "    axes.bar(range(1,6),scores)\n",
    "    print (\"The scores for random subset %s is: %s\" %(m+1,scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "As the figure shows that all of the performance on 5 folds are close to 1.\n",
    "\n",
    "After several times of random sufftle, the performance of each subsets are still close to 1.\n",
    "\n",
    "Therefore, the model work well on all random subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) but using cross validation. Use error bars on the features which are the standard deviation of the \n",
    "coefficiens obtained above. For this problem I\"ll walk you through the code. You just need to apply your optimal\n",
    "$\\alpha$ found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "def run_cv_coeffs(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    coeffs=[]\n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        coeffs.append(clf.coef_)\n",
    "    return coeffs\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X2 = X.as_matrix().astype(np.float)\n",
    "X2 = scaler.fit_transform(X)\n",
    "\n",
    "coeffs=run_cv_coeffs(X2,np.array(y),Lasso,alpha=alpha_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coeffs(coeffs):\n",
    "    coeffs_avgd = [(coeffs[0][i] + coeffs[1][i] + coeffs[2][i] + coeffs[3][i] + coeffs[4][i])/5 for i in range(0,len(X.columns))]\n",
    "    coeffs_std = [np.std([coeffs[0][i],coeffs[1][i],coeffs[2][i],coeffs[3][i],coeffs[4][i]]) for i in range(0,len(X.columns))]\n",
    "    return coeffs_avgd, coeffs_std\n",
    "coeffs_avg,coeffs_std=get_coeffs(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfCoeffs = pd.DataFrame({'type':X.columns.values, 'coef':coeffs_avg, 'std':coeffs_std})\n",
    "dfCoeffs = dfCoeffs[(dfCoeffs['coef']>1) |(dfCoeffs['coef']<-1) ]\n",
    "plt.figure(figsize=(15,15))\n",
    "dfCoeffs_sorted = dfCoeffs.sort(['coef'])[::-1]\n",
    "yerr_vals = dfCoeffs_sorted['std'].values\n",
    "dfCoeffs_sorted.plot(x='type',y='coef',kind='bar',yerr=yerr_vals,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
